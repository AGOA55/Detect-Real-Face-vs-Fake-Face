import cv2
import os
import numpy as np
import pickle
import shutil
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from scipy.spatial import distance as dist
import mediapipe as mp
from mtcnn import MTCNN
from keras_facenet import FaceNet
import serial
import serial.tools.list_ports
import time
import threading
import queue
from concurrent.futures import ThreadPoolExecutor
import multiprocessing as mp_process
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics.pairwise import cosine_similarity
import json
from datetime import datetime

# --- JSON Serializer Helper ---
class NumpyEncoder(json.JSONEncoder):
    """Custom JSON encoder for numpy data types"""
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)

# --- GPU Configuration ---
def setup_gpu():
    """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GPU ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö TensorFlow"""
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ memory growth ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ GPU memory overflow
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            
            # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ mixed precision ‡πÄ‡∏û‡∏∑‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
            tf.config.optimizer.set_jit(True)
            
            print(f"‚úÖ Found {len(gpus)} GPU(s): {[gpu.name for gpu in gpus]}")
            print("‚úÖ GPU memory growth enabled")
            return True
        except RuntimeError as e:
            print(f"‚ö† GPU setup error: {e}")
            return False
    else:
        print("‚ö† No GPU found, using CPU only")
        return False

# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GPU
GPU_AVAILABLE = setup_gpu()

# --- Global Settings ---
dataset_path = 'dataset'
embeddings_path = 'face_embeddings.pkl'
model_info_path = 'model_info.json'
liveness_model_path = 'DeepPixWeights.hdf5'  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• liveness
camera_index = 0
img_size = (160, 160)  # FaceNet input
liveness_size = (224, 224)  # Liveness model input
confidence_threshold = 0.75  # FaceNet similarity
liveness_threshold = 0.5    # DeepPix threshold

# --- Initializing Models ---
print("üîÑ Loading models...")
detector = MTCNN()  # CPU-based face detection
facenet_model = FaceNet()

# Load liveness detection model if available
liveness_model = None
if os.path.exists(liveness_model_path):
    try:
        print("üìÇ Loading Liveness model...")
        liveness_model = load_model(liveness_model_path)
        print("‚úÖ Liveness Detection Model Loaded")
    except Exception as e:
        print(f"‚ö† Failed to load liveness model: {e}")
        liveness_model = None
else:
    print("‚ö† Liveness model not found. Using MediaPipe liveness detection only.")

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ FaceNet ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ GPU
if GPU_AVAILABLE:
    with tf.device('/GPU:0'):
        # Warm up the model
        dummy_input = np.random.random((1, 160, 160, 3)).astype(np.float32)
        _ = facenet_model.embeddings([dummy_input])
        print("‚úÖ FaceNet Model Loaded on GPU")
else:
    print("‚úÖ FaceNet Model Loaded on CPU")

# MediaPipe setup
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(max_num_faces=4, refine_landmarks=True, 
                                  min_detection_confidence=0.5, min_tracking_confidence=0.5)

# Eye indices
EYE_INDICES_LEFT = [362, 385, 387, 263, 373, 380]
EYE_INDICES_RIGHT = [33, 160, 158, 133, 153, 144]

# --- Liveness Detection Functions ---
def check_liveness_deeppix(face_img):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö liveness ‡∏î‡πâ‡∏ß‡∏¢ DeepPix model"""
    if liveness_model is None:
        return True, 1.0  # fallback to True if no model
    
    try:
        face_resized = cv2.resize(face_img, liveness_size)
        face_resized = face_resized.astype("float32") / 255.0
        face_resized = np.expand_dims(face_resized, axis=0)
        pred = liveness_model.predict(face_resized, verbose=0)[0][0]
        return pred > liveness_threshold, pred
    except Exception as e:
        print(f"DeepPix error: {e}")
        return True, 1.0

def check_liveness_combined(face_img, mediapipe_verified=False, ear_score=0.0):
    """Combined liveness detection using both DeepPix and MediaPipe"""
    # DeepPix liveness
    deeppix_real, deeppix_score = check_liveness_deeppix(face_img)
    
    # MediaPipe liveness (blink + movement)
    mediapipe_weight = 0.6 if mediapipe_verified else 0.2
    deeppix_weight = 0.7 if liveness_model else 0.4
    
    # Combined score
    combined_score = (deeppix_score * deeppix_weight) + (mediapipe_weight if mediapipe_verified else 0.0)
    
    # Final decision
    is_real = (deeppix_real and deeppix_score > liveness_threshold) or mediapipe_verified
    
    return is_real, combined_score, deeppix_score

# --- Advanced Image Augmentation ---
def advanced_augmentation(image):
    """‡∏Å‡∏≤‡∏£ augment ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"""
    augmented_images = [image]  # ‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    
    # 1. Brightness adjustment
    bright_img = cv2.convertScaleAbs(image, alpha=1.2, beta=10)
    dark_img = cv2.convertScaleAbs(image, alpha=0.8, beta=-10)
    augmented_images.extend([bright_img, dark_img])
    
    # 2. Contrast adjustment
    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    
    # High contrast
    clahe_high = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
    l_high = clahe_high.apply(l)
    high_contrast = cv2.merge([l_high, a, b])
    high_contrast = cv2.cvtColor(high_contrast, cv2.COLOR_LAB2BGR)
    
    # Low contrast
    clahe_low = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(8,8))
    l_low = clahe_low.apply(l)
    low_contrast = cv2.merge([l_low, a, b])
    low_contrast = cv2.cvtColor(low_contrast, cv2.COLOR_LAB2BGR)
    
    augmented_images.extend([high_contrast, low_contrast])
    
    # 3. Gaussian noise
    noise = np.random.randint(0, 25, image.shape, dtype=np.uint8)
    noisy_img = cv2.add(image, noise)
    augmented_images.append(noisy_img)
    
    # 4. Gaussian blur
    blurred_img = cv2.GaussianBlur(image, (3,3), 1.0)
    augmented_images.append(blurred_img)
    
    # 5. Histogram equalization
    yuv = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)
    yuv[:,:,0] = cv2.equalizeHist(yuv[:,:,0])
    eq_img = cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)
    augmented_images.append(eq_img)
    
    return augmented_images

def quality_check_image(image):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û"""
    if image is None or image.size == 0:
        return False, "Empty image"
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡πà‡∏≤‡∏á
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    brightness = np.mean(gray)
    
    if brightness < 30:
        return False, "Too dark"
    elif brightness > 220:
        return False, "Too bright"
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏°‡∏ä‡∏±‡∏î (Laplacian variance)
    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
    if laplacian_var < 50:
        return False, "Too blurry"
    
    return True, "Good quality"

# --- Face Tracker Class ---
class FaceTracker:
    """‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ geometric matching"""
    def __init__(self, max_faces=4, max_disappeared=10):
        self.next_id = 0
        self.faces = {}
        self.disappeared = {}
        self.max_disappeared = max_disappeared
        self.max_faces = max_faces
    
    def register(self, centroid):
        """‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà"""
        self.faces[self.next_id] = centroid
        self.disappeared[self.next_id] = 0
        self.next_id += 1
        return self.next_id - 1
    
    def deregister(self, face_id):
        """‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"""
        del self.faces[face_id]
        del self.disappeared[face_id]
    
    def update(self, face_boxes):
        """‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤"""
        if len(face_boxes) == 0:
            # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤ - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
            for face_id in list(self.disappeared.keys()):
                self.disappeared[face_id] += 1
                if self.disappeared[face_id] > self.max_disappeared:
                    self.deregister(face_id)
            return {}
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì centroid ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤
        input_centroids = []
        for (x, y, w, h) in face_boxes:
            cx = x + w // 2
            cy = y + h // 2
            input_centroids.append((cx, cy))
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° - ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        if len(self.faces) == 0:
            face_assignments = {}
            for i, centroid in enumerate(input_centroids):
                face_id = self.register(centroid)
                face_assignments[face_id] = i
            return face_assignments
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡∏°‡πà
        face_centroids = list(self.faces.values())
        face_ids = list(self.faces.keys())
        
        # Hungarian algorithm ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢
        D = dist.cdist(np.array(face_centroids), np.array(input_centroids))
        rows = D.min(axis=1).argsort()
        cols = D.argmin(axis=1)[rows]
        
        used_row_indices = set()
        used_col_indices = set()
        face_assignments = {}
        
        # ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
        for (row, col) in zip(rows, cols):
            if row in used_row_indices or col in used_col_indices:
                continue
            
            # ‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà
            if D[row, col] > 100:  # threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°
                continue
            
            face_id = face_ids[row]
            self.faces[face_id] = input_centroids[col]
            self.disappeared[face_id] = 0
            face_assignments[face_id] = col
            
            used_row_indices.add(row)
            used_col_indices.add(col)
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà
        unused_rows = set(range(0, D.shape[0])).difference(used_row_indices)
        unused_cols = set(range(0, D.shape[1])).difference(used_col_indices)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà
        if D.shape[0] >= D.shape[1]:
            for row in unused_rows:
                face_id = face_ids[row]
                self.disappeared[face_id] += 1
                if self.disappeared[face_id] > self.max_disappeared:
                    self.deregister(face_id)
        
        # ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà
        else:
            for col in unused_cols:
                if len(self.faces) < self.max_faces:
                    face_id = self.register(input_centroids[col])
                    face_assignments[face_id] = col
        
        return face_assignments

# --- Background Recognition Worker with Improved Tracking ---
class BackgroundRecognitionWorker:
    """Background worker ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPU recognition ‡∏û‡∏£‡πâ‡∏≠‡∏° confidence checking"""
    def __init__(self, embeddings):
        self.embeddings = embeddings
        self.input_queue = queue.Queue(maxsize=3)
        self.output_queue = queue.Queue(maxsize=3)
        self.running = False
        self.thread = None
        self.similarity_threshold = 0.75  # ‡πÄ‡∏û‡∏¥‡πà‡∏° threshold
        self.confidence_threshold = 0.8   # threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á FaceNet model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö thread ‡∏ô‡∏µ‡πâ
        if GPU_AVAILABLE:
            with tf.device('/GPU:0'):
                self.facenet_model = FaceNet()
        else:
            self.facenet_model = FaceNet()
    
    def start(self):
        self.running = True
        self.thread = threading.Thread(target=self._worker)
        self.thread.daemon = True
        self.thread.start()
    
    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join()
    
    def _worker(self):
        while self.running:
            try:
                data = self.input_queue.get(timeout=0.1)
                face_rois, face_track_ids = data
                
                results = {}
                if face_rois:
                    # GPU processing
                    if GPU_AVAILABLE:
                        with tf.device('/GPU:0'):
                            batch_embeddings = self.facenet_model.embeddings(face_rois)
                    else:
                        batch_embeddings = self.facenet_model.embeddings(face_rois)
                    
                    for embedding, track_id in zip(batch_embeddings, face_track_ids):
                        best_match_name = "Unknown"
                        best_similarity = 0
                        second_best_similarity = 0
                        
                        # ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ similarity ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏•‡∏á‡∏°‡∏≤
                        similarities = []
                        for name, stored_embedding in self.embeddings.items():
                            similarity = 1 - dist.cosine(embedding, stored_embedding)
                            similarities.append((similarity, name))
                        
                        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á similarity ‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢
                        similarities.sort(reverse=True)
                        
                        if len(similarities) > 0:
                            best_similarity, best_match_name = similarities[0]
                            if len(similarities) > 1:
                                second_best_similarity = similarities[1][0]
                        
                        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç
                        confidence_gap = best_similarity - second_best_similarity
                        is_confident = (best_similarity > self.similarity_threshold and 
                                      confidence_gap > 0.1)  # ‡∏ï‡πâ‡∏≠‡∏á‡∏´‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 2 ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 0.1
                        
                        if is_confident and best_similarity > self.confidence_threshold:
                            status = "Pass"
                        else:
                            status = "Unknown"
                            best_match_name = "Unknown"
                            # ‡∏ñ‡πâ‡∏≤ confidence ‡∏ï‡πà‡∏≥ ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á similarity ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á
                            if best_similarity > 0.7:  # ‡∏°‡∏µ similarity ‡∏ö‡πâ‡∏≤‡∏á‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏û‡∏≠
                                best_match_name = f"Low_Conf_{best_similarity:.2f}"
                        
                        results[track_id] = {
                            'name': best_match_name,
                            'confidence': best_similarity,
                            'status': status,
                            'confidence_gap': confidence_gap
                        }
                
                self.output_queue.put(results)
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"Recognition error: {e}")
                self.output_queue.put({})
    
    def process_async(self, face_rois, face_track_ids):
        try:
            # ‡∏•‡πâ‡∏≤‡∏á queue ‡πÄ‡∏Å‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡πÄ‡∏ï‡πá‡∏°
            while not self.input_queue.empty():
                try:
                    self.input_queue.get_nowait()
                except queue.Empty:
                    break
            
            self.input_queue.put_nowait((face_rois, face_track_ids))
            return True
        except queue.Full:
            return False
    
    def get_result(self):
        try:
            return self.output_queue.get_nowait()
        except queue.Empty:
            return {}

# --- Helper Functions ---
def eye_aspect_ratio(eye):
    A = dist.euclidean(eye[1], eye[5])
    B = dist.euclidean(eye[2], eye[4])
    C = dist.euclidean(eye[0], eye[3])
    ear = (A + B) / (2.0 * C)
    return ear

def match_face_to_landmark(face_box, landmarks_list, frame_shape):
    """‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≤‡∏Å MTCNN ‡∏Å‡∏±‡∏ö landmarks ‡∏à‡∏≤‡∏Å MediaPipe"""
    h, w = frame_shape[:2]
    face_center = np.array([face_box[0] + face_box[2]/2, face_box[1] + face_box[3]/2])
    
    best_match_idx = -1
    min_distance = float('inf')
    
    for idx, face_landmarks in enumerate(landmarks_list):
        nose_tip = np.array([face_landmarks.landmark[1].x * w, face_landmarks.landmark[1].y * h])
        distance = np.linalg.norm(face_center - nose_tip)
        
        if distance < min_distance:
            min_distance = distance
            best_match_idx = idx
    
    if min_distance > 100:
        return -1
    
    return best_match_idx

def apply_clahe_color(img):
    """Image preprocessing with CPU optimization"""
    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    cl = clahe.apply(l)
    lab = cv2.merge((cl,a,b))
    img_clahe = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
    
    img_blur = cv2.GaussianBlur(img_clahe, (3,3), 0)
    gray = cv2.cvtColor(img_blur, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 50, 150)
    edges_3ch = cv2.merge([edges, edges, edges])
    edges_3ch = cv2.normalize(edges_3ch, None, 0, 255, cv2.NORM_MINMAX)
    final_img = cv2.addWeighted(img_blur, 0.7, edges_3ch, 0.3, 0)
    
    return final_img

# --- 1. Enhanced Capture Faces (50 images per angle) ---
def capture_faces():
    person_name = input("Enter person's name (English, no spaces): ").strip()
    if not person_name:
        print("‚ùå Invalid name.")
        return
    save_path = os.path.join(dataset_path, person_name)
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    
    cap = cv2.VideoCapture(camera_index)
    if not cap.isOpened():
        print(f"‚ùå Error: Could not open webcam.")
        return
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ï‡πà‡∏≠‡∏°‡∏∏‡∏°‡πÄ‡∏õ‡πá‡∏ô 50
    angles = ["Center", "Left", "Right", "Up", "Down"]
    samples_per_angle = 50  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 30 ‡πÄ‡∏õ‡πá‡∏ô 50
    
    # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    total_captured = 0
    rejected_count = 0
    
    for angle in angles:
        print(f"\nüì∏ Look {angle}. Press 's' to skip angle, 'q' to quit.")
        print(f"Target: {samples_per_angle} high-quality images")
        count = 0
        frame_count = 0
        
        while count < samples_per_angle:
            ret, frame = cap.read()
            if not ret: break
            
            frame_count += 1

            faces = detector.detect_faces(frame)
            if len(faces) > 0:
                x, y, w, h = faces[0]['box']
                x1, y1 = abs(x), abs(y)
                x2, y2 = x1 + w, y1 + h
                face_img = frame[y1:y2, x1:x2]

                if face_img.size > 0:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
                    is_good_quality, quality_msg = quality_check_image(face_img)
                    
                    # ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏π‡∏õ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ‡πÅ‡∏•‡∏∞‡∏ó‡∏∏‡∏Å 3 ‡πÄ‡∏ü‡∏£‡∏°
                    if is_good_quality and frame_count % 3 == 0:
                        face_img_resized = cv2.resize(face_img, img_size)
                        angle_folder = os.path.join(save_path, angle)
                        if not os.path.exists(angle_folder):
                            os.makedirs(angle_folder)
                        
                        filename = f"{count+1:03d}.jpg"  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô 3 ‡∏´‡∏•‡∏±‡∏Å
                        cv2.imwrite(os.path.join(angle_folder, filename), face_img_resized)
                        count += 1
                        total_captured += 1
                        
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                        text = f"{angle}: {count}/{samples_per_angle} ‚úì"
                    else:
                        rejected_count += 1
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 165, 255), 2)
                        text = f"{angle}: {count}/{samples_per_angle} - {quality_msg}"
                    
                    cv2.putText(frame, text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    
                    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
                    cv2.putText(frame, f"Captured: {total_captured} | Rejected: {rejected_count}", 
                               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    cv2.putText(frame, f"Quality: {quality_msg}", 
                               (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            else:
                cv2.putText(frame, "No face detected - Please position your face", 
                           (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
                cv2.putText(frame, f"Captured: {total_captured} | Rejected: {rejected_count}", 
                           (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

            cv2.imshow('Enhanced Face Capture', frame)
            
            key = cv2.waitKey(50) & 0xFF
            if key == ord('q'):
                cap.release()
                cv2.destroyAllWindows()
                return
            elif key == ord('s'):
                print(f"Skipping {angle} angle (captured {count} images)")
                break

    cap.release()
    cv2.destroyAllWindows()
    
    print(f"\n‚úÖ Finished capturing faces for '{person_name}'")
    print(f"üìä Total captured: {total_captured} images")
    print(f"üìä Total rejected: {rejected_count} images")
    print(f"üìä Quality ratio: {total_captured/(total_captured+rejected_count)*100:.1f}%")

# --- 2. Advanced Generate Embeddings ---
def generate_embeddings():
    print("\nüîÑ Advanced Face Embeddings Generation...")
    print("=" * 60)
    
    if not os.path.exists(dataset_path) or not os.listdir(dataset_path):
        print("‚ùå Dataset folder is empty. Please capture faces first.")
        return

    # ‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    stats = {
        'total_images': 0,
        'processed_images': 0,
        'augmented_images': 0,
        'persons': 0,
        'processing_time': 0,
        'timestamp': datetime.now().isoformat()
    }
    
    start_time = time.time()
    embeddings = {}
    all_images = []
    all_labels = []
    person_image_counts = {}
    
    print("üìÅ Scanning dataset...")
    
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô
    for folder_name in os.listdir(dataset_path):
        person_path = os.path.join(dataset_path, folder_name)
        if not os.path.isdir(person_path): continue
        
        person_image_counts[folder_name] = 0
        stats['persons'] += 1
        
        for angle_folder in os.listdir(person_path):
            angle_path = os.path.join(person_path, angle_folder)
            if not os.path.isdir(angle_path): continue
            
            image_files = [f for f in os.listdir(angle_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            person_image_counts[folder_name] += len(image_files)
            stats['total_images'] += len(image_files)
    
    print(f"üìä Found {stats['persons']} persons with {stats['total_images']} total images")
    print("üìä Images per person:")
    for person, count in person_image_counts.items():
        print(f"   ‚Ä¢ {person}: {count} images")
    
    print("\nüîÑ Processing images with advanced augmentation...")
    
    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô
    for folder_name in os.listdir(dataset_path):
        person_path = os.path.join(dataset_path, folder_name)
        if not os.path.isdir(person_path): continue

        person_images = []
        person_processed = 0
        
        print(f"\nüë§ Processing {folder_name}...")
        
        for angle_folder in os.listdir(person_path):
            angle_path = os.path.join(person_path, angle_folder)
            if not os.path.isdir(angle_path): continue
            
            print(f"   üìê Processing {angle_folder} angle...")
            angle_processed = 0

            for filename in os.listdir(angle_path):
                if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):
                    continue
                    
                img_path = os.path.join(angle_path, filename)
                image = cv2.imread(img_path)
                if image is None: continue

                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
                is_good_quality, quality_msg = quality_check_image(image)
                if not is_good_quality:
                    print(f"      ‚ö† Skipped {filename}: {quality_msg}")
                    continue

                # Apply advanced preprocessing
                image_clahe = apply_clahe_color(image)
                
                # Advanced augmentation (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ)
                augmented_images = advanced_augmentation(image_clahe)
                
                for aug_img in augmented_images:
                    image_rgb = cv2.cvtColor(aug_img, cv2.COLOR_BGR2RGB)
                    all_images.append(image_rgb)
                    all_labels.append(folder_name)
                    person_images.append(image_rgb)
                
                angle_processed += 1
                person_processed += 1
                stats['processed_images'] += 1
                stats['augmented_images'] += len(augmented_images)
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤
                if angle_processed % 10 == 0:
                    print(f"      ‚úì Processed {angle_processed} images from {angle_folder}")
        
        print(f"   ‚úÖ {folder_name}: {person_processed} original images ‚Üí {len(person_images)} total samples")

    if not all_images:
        print("‚ùå No valid images found to generate embeddings.")
        return

    print(f"\nüöÄ Generating embeddings for {len(all_images)} total samples...")
    print(f"üìä Augmentation ratio: {stats['augmented_images']/stats['processed_images']:.1f}x")
    
    # Batch processing with progress tracking
    batch_size = 32 if GPU_AVAILABLE else 8
    all_embeddings = []
    
    if GPU_AVAILABLE:
        with tf.device('/GPU:0'):
            for i in range(0, len(all_images), batch_size):
                batch = all_images[i:i+batch_size]
                batch_embeddings = facenet_model.embeddings(batch)
                all_embeddings.extend(batch_embeddings)
                
                progress = min(i+batch_size, len(all_images))
                percentage = (progress / len(all_images)) * 100
                print(f"   üîÑ GPU Processing: {progress}/{len(all_images)} ({percentage:.1f}%)")
    else:
        for i, image in enumerate(all_images):
            embedding = facenet_model.embeddings([image])[0]
            all_embeddings.append(embedding)
            
            if (i + 1) % 50 == 0:
                percentage = ((i + 1) / len(all_images)) * 100
                print(f"   üîÑ CPU Processing: {i+1}/{len(all_images)} ({percentage:.1f}%)")

    print("\nüìä Computing person embeddings with quality analysis...")
    
    # Group embeddings by person ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
    person_embeddings = {}
    person_stats = {}
    
    for embedding, label in zip(all_embeddings, all_labels):
        if label not in person_embeddings:
            person_embeddings[label] = []
            person_stats[label] = {'count': 0, 'avg_norm': 0.0, 'std_norm': 0.0}
        
        # Normalize embedding
        normalized_embedding = embedding / np.linalg.norm(embedding)
        person_embeddings[label].append(normalized_embedding)
        person_stats[label]['count'] += 1

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì final embeddings ‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    for person, emb_list in person_embeddings.items():
        embeddings[person] = np.mean(emb_list, axis=0)
        embeddings[person] /= np.linalg.norm(embeddings[person])
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Python native types
        norms = [np.linalg.norm(emb) for emb in emb_list]
        person_stats[person]['avg_norm'] = float(np.mean(norms))  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô float
        person_stats[person]['std_norm'] = float(np.std(norms))   # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô float
        
        print(f"   üë§ {person}: {person_stats[person]['count']} samples, "
              f"norm={person_stats[person]['avg_norm']:.3f}¬±{person_stats[person]['std_norm']:.3f}")

    # Cross-person similarity analysis
    print("\nüîç Cross-person similarity analysis:")
    person_names = list(embeddings.keys())
    similarity_matrix = {}
    
    if len(person_names) > 1:
        for i in range(len(person_names)):
            for j in range(i+1, len(person_names)):
                similarity = float(1 - dist.cosine(embeddings[person_names[i]], embeddings[person_names[j]]))  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô float
                similarity_key = f"{person_names[i]}_vs_{person_names[j]}"
                similarity_matrix[similarity_key] = similarity
                
                print(f"   üìè {person_names[i]} ‚Üî {person_names[j]}: {similarity:.3f}")
                
                if similarity > 0.7:
                    print(f"      ‚ö† High similarity detected! May cause confusion.")

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    with open(embeddings_path, 'wb') as f:
        pickle.dump(embeddings, f)

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç JSON serialization)
    stats['processing_time'] = float(time.time() - start_time)  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô float
    stats['person_stats'] = person_stats
    stats['similarity_matrix'] = similarity_matrix
    
    model_info = {
        'version': '2.0',
        'model_type': 'FaceNet + Advanced Augmentation + Liveness',
        'stats': stats,
        'settings': {
            'similarity_threshold': 0.75,
            'confidence_threshold': 0.82,
            'liveness_threshold': liveness_threshold,
            'augmentation_ratio': float(stats['augmented_images']/stats['processed_images']),  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô float
            'gpu_enabled': GPU_AVAILABLE,
            'liveness_model_available': liveness_model is not None
        }
    }
    
    # ‡πÉ‡∏ä‡πâ NumpyEncoder ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ numpy types
    with open(model_info_path, 'w') as f:
        json.dump(model_info, f, indent=2, cls=NumpyEncoder)

    print("\n" + "="*60)
    print("‚úÖ Advanced embeddings generation completed!")
    print(f"üìä Processing time: {stats['processing_time']:.1f} seconds")
    print(f"üìä Processing speed: {len(all_images)/stats['processing_time']:.1f} images/second")
    print(f"üìÅ Embeddings saved to: {embeddings_path}")
    print(f"üìÅ Model info saved to: {model_info_path}")

def find_arduino_port(baudrate=9600, timeout=2):
    keywords = ["arduino", "ch340", "usb serial", "usb-serial", "ch9102", "silicon labs"]
    ports = serial.tools.list_ports.comports()

    for port in ports:
        desc = port.description.lower()
        if any(keyword in desc for keyword in keywords):
            try:
                ser = serial.Serial(port.device, baudrate, timeout=1)
                ser.close()
                return port.device
            except Exception:
                pass

    for port in ports:
        try:
            ser = serial.Serial(port.device, baudrate, timeout=1)
            ser.close()
            return port.device
        except Exception:
            pass

    return None

# --- 3. Improved Face Recognition with Liveness Detection ---
def recognize_faces():
    try:
        with open(embeddings_path, 'rb') as f:
            embeddings = pickle.load(f)
            
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•
        model_info = {}
        if os.path.exists(model_info_path):
            with open(model_info_path, 'r') as f:
                model_info = json.load(f)
                print(f"üìä Loaded model: {model_info.get('model_type', 'Unknown')}")
                print(f"üìä Dataset: {model_info.get('stats', {}).get('persons', 0)} persons, "
                      f"{model_info.get('stats', {}).get('total_images', 0)} images")
                print(f"üìä Liveness: {'DeepPix + MediaPipe' if liveness_model else 'MediaPipe Only'}")
    except FileNotFoundError:
        print("‚ùå Embeddings file not found. Please generate embeddings first.")
        return

    # Arduino setup
    arduino_port = find_arduino_port()
    if arduino_port:
        try:
            ser = serial.Serial(arduino_port, 9600, timeout=1)
            print(f"üîå Connected to Arduino on {arduino_port}")
        except Exception as e:
            print(f"‚ùå Could not open serial port: {e}")
            ser = None
    else:
        print("‚ö† No Arduino found.")
        ser = None

    cap = cv2.VideoCapture(camera_index)
    if not cap.isOpened():
        print("‚ùå Error: Could not open webcam.")
        return

    # Initialize trackers
    face_tracker = FaceTracker(max_faces=4, max_disappeared=20)
    recognition_worker = BackgroundRecognitionWorker(embeddings)
    recognition_worker.start()

    print(f"\nüîÑ Advanced recognition with liveness detection started.")
    print(f"üîß Processing: {'GPU+CPU' if GPU_AVAILABLE else 'CPU Only'}")
    print(f"üîß Liveness: {'DeepPix + MediaPipe' if liveness_model else 'MediaPipe Only'}")
    
    # State tracking ‡πÉ‡∏ä‡πâ track_id ‡πÅ‡∏ó‡∏ô face_idx
    EAR_THRESHOLD, EAR_CONSEC_FRAMES = 0.2, 3
    MOVE_THRESHOLD = 7
    
    person_states = {}
    recognition_cache = {}
    liveness_cache = {}  # ‡πÄ‡∏û‡∏¥‡πà‡∏° cache ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö liveness
    frame_count = 0

    try:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break
            
            frame_count += 1
            h, w, _ = frame.shape
            
            # --- Synchronous Processing ---
            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            mp_results = face_mesh.process(image_rgb)
            faces = detector.detect_faces(frame)

            if not faces:
                # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤ - ‡∏•‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°
                face_assignments = face_tracker.update([])
                cv2.putText(frame, "No faces detected", (50, 50), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                
                if ser:
                    ser.write("No\n".encode())
                
                cv2.imshow('Advanced Face Recognition + Liveness', frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
                continue

            # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó face tracking
            face_boxes = [face['box'] for face in faces]
            face_assignments = face_tracker.update(face_boxes)

            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö background recognition
            if frame_count % 5 == 0 and face_assignments:
                face_rois = []
                face_track_ids = []
                
                for track_id, face_idx in face_assignments.items():
                    if face_idx < len(faces):
                        x_min, y_min, w_mtcnn, h_mtcnn = faces[face_idx]['box']
                        x_max, y_max = x_min + w_mtcnn, y_min + h_mtcnn
                        
                        face_roi = frame[y_min:y_max, x_min:x_max]
                        if face_roi.size > 0:
                            face_roi_processed = apply_clahe_color(face_roi)
                            face_roi_resized = cv2.resize(face_roi_processed, img_size)
                            face_roi_rgb = cv2.cvtColor(face_roi_resized, cv2.COLOR_BGR2RGB)
                            
                            face_rois.append(face_roi_rgb)
                            face_track_ids.append(track_id)
                
                if face_rois:
                    recognition_worker.process_async(face_rois, face_track_ids)

            # ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å background worker
            new_results = recognition_worker.get_result()
            if new_results:
                # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó cache ‡πÅ‡∏ï‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞ track_id ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
                valid_track_ids = set(face_assignments.keys())
                # ‡∏•‡∏ö cache ‡∏Ç‡∏≠‡∏á track_id ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                recognition_cache = {tid: result for tid, result in recognition_cache.items() 
                                   if tid in valid_track_ids}
                liveness_cache = {tid: result for tid, result in liveness_cache.items() 
                                if tid in valid_track_ids}
                # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó cache ‡πÉ‡∏´‡∏°‡πà
                recognition_cache.update(new_results)

            # Process ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
            current_detections = []
            
            if mp_results and mp_results.multi_face_landmarks:
                for track_id, face_idx in face_assignments.items():
                    if face_idx >= len(faces):
                        continue
                        
                    x_min, y_min, w_mtcnn, h_mtcnn = faces[face_idx]['box']
                    x_max, y_max = x_min + w_mtcnn, y_min + h_mtcnn
                    face_roi = frame[y_min:y_max, x_min:x_max]
                    
                    # ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà landmarks
                    landmark_idx = match_face_to_landmark(faces[face_idx]['box'], mp_results.multi_face_landmarks, (h, w))
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á state ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö track_id ‡πÉ‡∏´‡∏°‡πà
                    if track_id not in person_states:
                        person_states[track_id] = {
                            'blink_counter': 0,
                            'blink_verified': False,
                            'movement_verified': False,
                            'prev_nose_pos': None,
                            'verified': False,
                            'mediapipe_liveness': False
                        }
                    
                    # Liveness Detection
                    if face_roi.size > 0 and frame_count % 10 == 0:  # ‡∏ó‡∏∏‡∏Å 10 ‡πÄ‡∏ü‡∏£‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö liveness
                        mediapipe_verified = person_states[track_id]['verified']
                        is_real, combined_score, deeppix_score = check_liveness_combined(
                            face_roi, mediapipe_verified, 0.0)
                        
                        liveness_cache[track_id] = {
                            'is_real': is_real,
                            'combined_score': combined_score,
                            'deeppix_score': deeppix_score,
                            'mediapipe_verified': mediapipe_verified
                        }
                    
                    if landmark_idx >= 0:
                        face_landmarks = mp_results.multi_face_landmarks[landmark_idx]
                        landmarks = np.array([(lm.x * w, lm.y * h) for lm in face_landmarks.landmark], dtype=int)
                        
                        # MediaPipe Liveness detection
                        left_eye = np.array([landmarks[i] for i in EYE_INDICES_LEFT])
                        right_eye = np.array([landmarks[i] for i in EYE_INDICES_RIGHT])
                        ear = (eye_aspect_ratio(left_eye) + eye_aspect_ratio(right_eye)) / 2.0
                        
                        # Blink detection
                        if ear < EAR_THRESHOLD:
                            person_states[track_id]['blink_counter'] += 1
                        else:
                            if person_states[track_id]['blink_counter'] >= EAR_CONSEC_FRAMES:
                                person_states[track_id]['blink_verified'] = True
                            person_states[track_id]['blink_counter'] = 0
                        
                        # Movement detection
                        nose_tip = landmarks[1]
                        if person_states[track_id]['prev_nose_pos'] is not None:
                            move_dist = np.linalg.norm(np.array(nose_tip) - np.array(person_states[track_id]['prev_nose_pos']))
                            if move_dist > MOVE_THRESHOLD:
                                person_states[track_id]['movement_verified'] = True
                        person_states[track_id]['prev_nose_pos'] = nose_tip
                        
                        # MediaPipe Verification
                        person_states[track_id]['verified'] = (person_states[track_id]['blink_verified'] and 
                                                               person_states[track_id]['movement_verified'])
                        person_states[track_id]['mediapipe_liveness'] = person_states[track_id]['verified']
                        
                        # Combined Display Logic
                        display_text = "Blink & Move to Verify"
                        display_color = (0, 165, 255)  # Orange for verification
                        status = "No"
                        
                        # Get liveness result
                        liveness_result = liveness_cache.get(track_id, {
                            'is_real': False, 'combined_score': 0.0, 'deeppix_score': 0.0, 'mediapipe_verified': False
                        })
                        
                        if person_states[track_id]['verified'] and track_id in recognition_cache:
                            recognition_result = recognition_cache[track_id]
                            
                            # Combine recognition and liveness results
                            is_live = liveness_result['is_real']
                            live_score = liveness_result['combined_score']
                            
                            if recognition_result['status'] == "Pass" and is_live:
                                display_color = (0, 255, 0)  # Green for pass
                                status = "Pass"
                                display_text = f"{recognition_result['name']} ‚úìLive ({live_score:.2f})"
                            elif recognition_result['status'] == "Pass" and not is_live:
                                display_color = (0, 0, 255)  # Red for fake
                                status = "Fake"
                                display_text = f"{recognition_result['name']} ‚úóFake ({live_score:.2f})"
                            elif not is_live:
                                display_color = (0, 0, 255)  # Red for fake
                                status = "Fake"
                                display_text = f"Fake Detected ({live_score:.2f})"
                            else:
                                display_color = (0, 0, 255)  # Blue for unknown but live
                                status = "Unknown"
                                display_text = f"Unknown ‚úìLive ({live_score:.2f})"
                        
                        current_detections.append({
                            'box': (x_min, y_min, x_max, y_max),
                            'text': display_text,
                            'color': display_color,
                            'status': status,
                            'track_id': track_id,
                            'liveness_info': liveness_result
                        })

            # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
            for detection in current_detections:
                x_min, y_min, x_max, y_max = detection['box']
                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), detection['color'], 2)
                cv2.putText(frame, detection['text'], (x_min, y_min - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, detection['color'], 2)
                cv2.putText(frame, f"ID:{detection['track_id']}", (x_min, y_max + 20),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, detection['color'], 1)
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• liveness ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
                liveness_info = detection['liveness_info']
                if liveness_model:
                    liveness_detail = f"DeepPix:{liveness_info['deeppix_score']:.2f} MP:{liveness_info['mediapipe_verified']}"
                    cv2.putText(frame, liveness_detail, (x_min, y_max + 40),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, detection['color'], 1)

            # Send to Arduino
            if ser:
                status_list = [d['status'] for d in current_detections] or ["No"]
                status_msg = ",".join(status_list) + "\n"
                ser.write(status_msg.encode())

            # Performance info
            gpu_status = "GPU+CPU" if GPU_AVAILABLE else "CPU Only"
            liveness_status = "DeepPix+MP" if liveness_model else "MP Only"
            cv2.putText(frame, f"Mode: {gpu_status} + {liveness_status} | Faces: {len(current_detections)}", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(frame, f"Active Tracks: {len(face_tracker.faces)}", 
                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            cv2.imshow('Advanced Face Recognition + Liveness', frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    finally:
        # Cleanup
        print("üõë Stopping recognition...")
        recognition_worker.stop()
        cap.release()
        face_mesh.close()
        cv2.destroyAllWindows()
        if ser:
            ser.close()

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
def list_persons():
    if not os.path.exists(dataset_path):
        print("‚ùå Dataset folder does not exist.")
        return []
    
    persons = [name for name in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, name))]
    if not persons:
        print("‚ùå No persons found in the dataset.")
        return []
    
    print("\nüìã Persons in database:")
    total_images = 0
    
    for idx, name in enumerate(persons, 1):
        person_path = os.path.join(dataset_path, name)
        image_count = 0
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏°‡∏∏‡∏°
        for angle_folder in os.listdir(person_path):
            angle_path = os.path.join(person_path, angle_folder)
            if os.path.isdir(angle_path):
                image_files = [f for f in os.listdir(angle_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                image_count += len(image_files)
        
        total_images += image_count
        print(f"{idx}. {name} ({image_count} images)")
    
    print(f"\nüìä Total: {len(persons)} persons, {total_images} images")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if os.path.exists(model_info_path):
        with open(model_info_path, 'r') as f:
            model_info = json.load(f)
            print(f"üìä Model: {model_info.get('model_type', 'Unknown')}")
            print(f"üìä Last training: {model_info.get('stats', {}).get('timestamp', 'Unknown')}")
            print(f"üìä Liveness: {'Available' if model_info.get('settings', {}).get('liveness_model_available', False) else 'MediaPipe Only'}")
    
    return persons

def delete_person_data():
    persons = list_persons()
    if not persons:
        return
    try:
        choice = input("Enter the number of the person to delete (or 'c' to cancel): ").strip()
        if choice.lower() == 'c':
            print("Canceled deletion.")
            return
        choice_num = int(choice)
        if 1 <= choice_num <= len(persons):
            person_name = persons[choice_num - 1]
            person_path = os.path.join(dataset_path, person_name)
            confirm = input(f"Are you sure you want to delete all data for '{person_name}'? (y/n): ").strip().lower()
            if confirm == 'y':
                shutil.rmtree(person_path)
                print(f"‚úÖ Successfully deleted '{person_name}'. Please generate embeddings again.")
            else:
                print("Deletion canceled.")
        else:
            print("‚ùå Invalid number.")
    except ValueError:
        print("‚ùå Invalid input.")

def main():
    if not os.path.exists(dataset_path): 
        os.makedirs(dataset_path)
    
    while True:
        print("\n" + "="*60 + "\n   Advanced Face Recognition + Liveness System v2.0\n" + "="*60)
        print(f"üîß Hardware: {'GPU+CPU' if GPU_AVAILABLE else 'CPU Only'}")
        print(f"üîß Liveness: {'DeepPix + MediaPipe' if liveness_model else 'MediaPipe Only'}")
        print("1. Capture new faces (50 images per angle)")
        print("2. Generate Advanced Embeddings (GPU + Augmentation)")
        print("3. Start recognition with liveness detection")
        print("4. Show persons in database")
        print("5. Delete a person's data")
        print("6. Exit")
        
        choice = input("Enter your choice (1-6): ").strip()
        
        if choice == '1':
            capture_faces()
        elif choice == '2':
            generate_embeddings()
        elif choice == '3':
            recognize_faces()
        elif choice == '4':
            list_persons()
        elif choice == '5':
            delete_person_data()
        elif choice == '6':
            print("Exiting...")
            break
        else:
            print("‚ùå Invalid choice.")

if __name__ == "__main__":
    main()
